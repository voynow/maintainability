{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Union, Any, Optional\n",
    "import config, models\n",
    "import re\n",
    "from llm_blocks import block_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# TODO: logic for null \"volume\"s\n",
    "\n",
    "\n",
    "def first_last_edge_case(data):\n",
    "\n",
    "    \"\"\" Check for edge case of nan values in first or last position \"\"\"\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "\n",
    "        # first item in series is nan\n",
    "        if np.isnan(data[0, i]):\n",
    "            index = 1\n",
    "            while np.isnan(data[index, i]):\n",
    "                index += 1\n",
    "            data[0, i] = np.copy(data[index, i])\n",
    "\n",
    "        # last item in series is nan\n",
    "        if np.isnan(data[-1, i]):\n",
    "            index = 1\n",
    "            while np.isnan(data[-1 * index, i]):\n",
    "                index += 1\n",
    "            data[-1, i] = np.copy(data[-1 * index, i])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_subsets(nan_rows):\n",
    "\n",
    "    \"\"\" Find column-wise sets of nan values \"\"\"\n",
    "\n",
    "    max_value = np.max(nan_rows)\n",
    "    nan_sets = []\n",
    "    i = 0\n",
    "\n",
    "    # loop on i for all nan_rows in given column\n",
    "    while i < (nan_rows.shape[0] - 1):\n",
    "\n",
    "        current_set = [nan_rows[i]]\n",
    "\n",
    "        # possible nan row values to search for\n",
    "        possible_sequences = np.arange(nan_rows[i] + 1, max_value + 1)\n",
    "\n",
    "        # indexes for searching nan_rows and possible_sequences\n",
    "        subset_search_index = 0\n",
    "        nan_rows_search_index = i + 1\n",
    "\n",
    "        # loop through adjacent (by row position) nans values\n",
    "        while True and len(possible_sequences) > subset_search_index:\n",
    "\n",
    "            # if adjacent nan exists\n",
    "            if possible_sequences[subset_search_index] == nan_rows[nan_rows_search_index]:\n",
    "\n",
    "                # add to current set and increment index pointers\n",
    "                current_set.append(nan_rows[nan_rows_search_index])\n",
    "                subset_search_index += 1\n",
    "                nan_rows_search_index += 1\n",
    "                i += 1\n",
    "\n",
    "            # break loop if no adjacent nan exists\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        nan_sets.append(current_set)\n",
    "        i += 1\n",
    "\n",
    "    # edge case for adding last row\n",
    "    if i != nan_rows.shape[0]:\n",
    "        nan_sets.append([nan_rows[-1]])\n",
    "\n",
    "    return nan_sets\n",
    "\n",
    "\n",
    "def test_subsets(subsets_tensor, nan_rows_tensor):\n",
    "\n",
    "    \"\"\" Check conservation of all data through conversion to subsets \"\"\"\n",
    "\n",
    "    tests_passed = True\n",
    "\n",
    "    for i in range(len(subsets_tensor)):\n",
    "        items = []\n",
    "        for j in range(len(subsets_tensor[i])):\n",
    "            if len(subsets_tensor[i][j]) > 1:\n",
    "                for k in range(len(subsets_tensor[i][j])):\n",
    "                    items.append(subsets_tensor[i][j][k])\n",
    "            else:\n",
    "                items.append(subsets_tensor[i][j][0])\n",
    "\n",
    "        if len(items) != nan_rows_tensor[i].shape[0]:\n",
    "            print(\"Error: Tensors at i =\", i, \"have mismatched dimensions\")\n",
    "            tests_passed = False\n",
    "        else:\n",
    "            comparison_matrix = np.vstack((items, nan_rows_tensor[i])).T\n",
    "            difference = np.sum(np.diff(comparison_matrix, axis=1))\n",
    "            if difference:\n",
    "                print(\"Error: Subset i =\", i, \" is missing items from its associated nan_rows\")\n",
    "                tests_passed = False\n",
    "\n",
    "    if tests_passed:\n",
    "        print(\"1/2 Test Passed: All nan_rows preserved when converted to subsets_tensor\")\n",
    "\n",
    "\n",
    "def get_nan_subsets(raw_data):\n",
    "\n",
    "    \"\"\" Find sets of nan values in each timeseries \"\"\"\n",
    "\n",
    "    subsets_tensor = []\n",
    "    nan_rows_tensor = []\n",
    "\n",
    "    # iterate through columns, find nan positions\n",
    "    for i in range(raw_data.shape[1]):\n",
    "        nan_rows = np.where(np.isnan(raw_data[:, i]))[0]\n",
    "        nan_rows_tensor.append(nan_rows)\n",
    "\n",
    "        # convert nan rows into sets/strings of nan values\n",
    "        if len(nan_rows):\n",
    "            subsets = get_subsets(nan_rows)\n",
    "        else:\n",
    "            subsets = []\n",
    "        subsets_tensor.append(subsets)\n",
    "\n",
    "    # verify conservation of data\n",
    "    test_subsets(subsets_tensor, nan_rows_tensor)\n",
    "\n",
    "    return subsets_tensor\n",
    "\n",
    "\n",
    "def test_nan_fill(column, column_index, subset, first_index, last_index):\n",
    "    new_distances = np.diff(column[first_index - 1: last_index + 2])\n",
    "    error_threshold = column[subset[0]] / 10e3\n",
    "    difference_sum = np.sum(new_distances - new_distances[0])\n",
    "\n",
    "    if np.absolute(difference_sum) > error_threshold:\n",
    "        print(\"Error: Filling nan values at column =\", column_index, \"subset =\", subset)\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def nan_filler(raw_data, subsets_tensor):\n",
    "    tests_passed = True\n",
    "    for i in range(raw_data.shape[1]):\n",
    "        subsets = subsets_tensor[i]\n",
    "        column = raw_data[:, i]\n",
    "\n",
    "        for subset in subsets:\n",
    "            first_index = subset[0]\n",
    "            last_index = subset[-1]\n",
    "            subset_length = len(subset)\n",
    "\n",
    "            linspace_values = np.linspace(column[first_index - 1], column[last_index + 1], subset_length + 2)\n",
    "            column[first_index: last_index + 1] = linspace_values[1:-1]\n",
    "            raw_data[:, i] = column\n",
    "\n",
    "            if not test_nan_fill(column, i, subset, first_index, last_index):\n",
    "                tests_passed = False\n",
    "\n",
    "    if tests_passed:\n",
    "            print(\"2/2 Test Passed: All nan values are now numerical\")\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def fill_nans(df):\n",
    "\n",
    "    print(\"*\" * 58 + \"\\n\\tFilling NAN values in remaining series\\n\" + \"*\" * 58, \"\\n\")\n",
    "    datetime_data = df.index\n",
    "\n",
    "    # remove datetime column\n",
    "    raw_data = df.drop([df.columns[0]], axis=1).values\n",
    "\n",
    "    # check for first/last nan position edge case\n",
    "    raw_data = first_last_edge_case(raw_data)\n",
    "\n",
    "    # find an fill nan values\n",
    "    subsets_tensor = get_nan_subsets(raw_data)\n",
    "    raw_data = nan_filler(raw_data, subsets_tensor)\n",
    "\n",
    "    # convert data back to df\n",
    "    filled_df = pd.DataFrame(raw_data, columns=df.columns[1:])\n",
    "    filled_df.index = datetime_data\n",
    "\n",
    "    return filled_df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_llm() -> callable:\n",
    "    return block_factory.get(\n",
    "        \"template\",\n",
    "        template=config.PROMPT,\n",
    "        temperature=config.TEMPERATURE,\n",
    "        model_name=config.MODEL_NAME,\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_response(text: str) -> float:\n",
    "    try:\n",
    "        match = re.search(r\"(\\d{1,2})/10\", text)\n",
    "        response = int(match.group(1))\n",
    "    except AttributeError as e:\n",
    "        response = -1\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_maintainability_metrics(\n",
    "    filepath: str, code: str\n",
    ") -> models.MaintainabilityMetrics:\n",
    "    metric_collection = {}\n",
    "    gpt_interface = get_llm()\n",
    "    for metric, description in config.METRIC_DESCRIPTIONS.items():\n",
    "        metric_name_formatted = metric.replace(\"_\", \" \")\n",
    "        response = gpt_interface(\n",
    "            filepath=filepath,\n",
    "            code=code,\n",
    "            metric=metric_name_formatted,\n",
    "            description=description,\n",
    "        )\n",
    "        print(\"*\" * 100)\n",
    "        print(metric)\n",
    "        print(description)\n",
    "        print(\"LLM Response:\")\n",
    "        print(response)\n",
    "        print(\"*\" * 100)\n",
    "        metric_collection[metric] = parse_response(response)\n",
    "    return metric_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "intuitive_design\n",
      "Variable and Function Naming: Clear names indicating purpose and type.\n",
      "        Good: total_amount, calculate_tax(income)\n",
      "        Bad: ta, calc()\n",
      "        Comment Quality: Comments clarify the 'why,' not just the 'what.'\n",
      "        Good: \"Using binary search for performance.\"\n",
      "        Bad: \"Incrementing counter.\"\n",
      "        Code Organization: Logical structuring is key, extending to function and class organization.\n",
      "        Good: Methods in a class organized by functionality.\n",
      "        Bad: Disorganized mix of functions and classes.\n",
      "        API Usability: Includes intuitiveness, documentation quality, and new developer onboarding.\n",
      "        Good: Self-explanatory method names, documented parameters.\n",
      "        Bad: Poorly documented functions, ambiguous parameters.\n",
      "        Code Simplicity: Aim for straightforward code without sacrificing functionality.\n",
      "        Good: Using list comprehensions instead of simple for-loops.\n",
      "        Bad: Nested loops and if-statements that could be simplified.\n",
      "LLM Response:\n",
      "- The code seems to be well-structured with clear variable and function names, making it easy to understand the purpose of each component.\n",
      "- The comments provided are helpful in understanding the logic and reasoning behind certain code sections.\n",
      "- The code could benefit from more comprehensive error handling and input validation to handle potential edge cases.\n",
      "\n",
      "Overall, I would rate this code review as 8/10.\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "functional_cohesion\n",
      "Single Responsibility Principle: Each function or module should have one, and only one, reason to change. This means it should perform one logical task.\n",
      "        Good: A function named calculate_tax(income) that only calculates tax.\n",
      "        Bad: A function named process_user_data() that validates input, updates a database, and sends email.\n",
      "        Separation of Concerns: Different aspects of the program should be separated into distinct sections of the codebase. This involves following established architectural patterns.\n",
      "        Good: Using MVC to separate data handling from business logic and UI.\n",
      "        Bad: Functions that mix database queries, business logic, and UI updates.\n",
      "        Function Length: While not a hard rule, aim for shorter functions when possible to make the code more readable and maintainable.\n",
      "        Good: Functions that are less than 20 lines of code.\n",
      "        Bad: Functions that are several hundred lines long.\n",
      "        Module Cohesion: Functions within a module should be strongly related in functionality. Avoid \"god\" modules that do everything.\n",
      "        Good: A PaymentProcessing module that only contains functions related to payment.\n",
      "        Bad: A Utilities module that contains everything from string manipulation to network requests.\n",
      "LLM Response:\n",
      "- The code seems to be implementing a data cleaning process for handling missing values in a dataset.\n",
      "- The code is well-structured with functions that have clear responsibilities.\n",
      "- The use of numpy and pandas libraries indicates efficient data manipulation and analysis.\n",
      "\n",
      "Overall, I would rate this code a 7/10. It demonstrates good functional cohesion and follows the Single Responsibility Principle. However, there is room for improvement in terms of code readability and documentation.\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "adaptive_resilience\n",
      "Error-Handling: Employ comprehensive error-handling mechanisms, not just for external calls but also for internal logic that is prone to failure.\n",
      "        Good: Using try-except blocks around not just HTTP requests but also file operations, database calls, and any segments where exceptions might occur.\n",
      "        Bad: Only using error-handling for network requests but ignoring it for file I/O or other risky operations.\n",
      "        Graceful Degradation: Code should still function, albeit at a reduced level, even when some subsystems or services fail.\n",
      "        Good: Implementing circuit breakers or fallback methods when a dependent service is down.\n",
      "        Bad: No handling for scenarios where a dependent service is unavailable, leading to complete failure.\n",
      "        Resource Management: Explicitly manage all resources, not just file handles. This includes connections, memory, and even thread management.\n",
      "        Good: Using context managers like with in Python for file operations, database connections, and thread locks.\n",
      "        Bad: Leaving files open, database connections unclosed, or locks unreleased.\n",
      "        Adaptability: Ensure the code can adapt to different conditions, including load and data variability.\n",
      "        Good: Implementing auto-scaling, rate-limiting, and data partitioning to handle different operational scenarios.\n",
      "        Bad: Writing code that can't handle varying loads or data sizes.\n",
      "LLM Response:\n",
      "- The code lacks comprehensive error handling mechanisms. There are no try-except blocks to handle potential exceptions in various operations.\n",
      "- The code does not demonstrate graceful degradation. There is no fallback mechanism in case of failure in dependent services.\n",
      "- Resource management is not explicitly handled. There are no context managers used for file operations, database connections, or thread locks.\n",
      "\n",
      "Overall Score: 4/10\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "code_efficiency\n",
      "Algorithmic Complexity: Use efficient algorithms and data structures. Complexity worse than O(n logn) should be justified.\n",
      "        Good: Using quicksort or mergesort for sorting tasks.\n",
      "        Bad: Using bubble sort for a large dataset without justification.\n",
      "        Resource Utilization: Monitor CPU and memory usage and minimize their footprint.\n",
      "        Good: Utilizing list comprehensions or generators in Python for more efficient memory use.\n",
      "        Bad: Using regular loops that create additional variables and take up more memory for simple tasks.\n",
      "        Runtime Profiling: Actively profile code to identify bottlenecks.\n",
      "        Good: Using tools like cProfile or timeit in Python to find slow segments of code.\n",
      "        Bad: Not measuring performance or ignoring bottlenecks.\n",
      "        Concurrency: Make use of parallelism and asynchronous programming where appropriate.\n",
      "        Good: Using Python's asyncio for I/O-bound tasks or ThreadPoolExecutor for CPU-bound tasks.\n",
      "        Bad: Running all tasks sequentially in a single-threaded environment when concurrency could be beneficial.\n",
      "        Data Fetching and Caching: Optimize how data is retrieved and stored.\n",
      "        Good: Implementing caching mechanisms or using batch retrieval for database calls.\n",
      "        Bad: Making frequent, repetitive calls to a database for the same data.\n",
      "LLM Response:\n",
      "- The code seems to be written in Python and uses libraries such as pandas and numpy for data manipulation and analysis.\n",
      "- The code includes several functions that perform different tasks such as handling null values, finding subsets of null values, and filling null values.\n",
      "- The code lacks proper documentation and comments, making it difficult to understand the purpose and functionality of each function.\n",
      "\n",
      "Overall, I would rate the code a 6/10. While it seems to perform the required tasks, the lack of documentation and comments makes it difficult to evaluate its efficiency and correctness. Additionally, there is no information provided about the expected input and output of each function, which further hinders the code review process.\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "data_security_and_integrity\n",
      "Data Validation: Validate all user inputs at both client and server sides. This is not just about format but also about acceptable ranges or values.\n",
      "        Good: Using regex to validate usernames and also checking for banned or reserved names.\n",
      "        Bad: Only using front-end validation, which can be bypassed.\n",
      "        Data Sanitation: Ensure that all data is sanitized to prevent injection attacks, and not just SQL injections.\n",
      "        Good: Using parameterized SQL queries, HTML entity encoding for web views.\n",
      "        Bad: Concatenating user inputs directly into SQL queries or HTML views.\n",
      "        Password and API Key Security: All sensitive information should not just be encrypted but also securely stored and transmitted.\n",
      "        Good: Storing hashed passwords using strong algorithms like bcrypt, and keeping API keys in environment variables or secure key vaults.\n",
      "        Bad: Storing passwords in plaintext or keeping API keys hard-coded in the codebase.\n",
      "        Data Integrity Checks: Implement controls to ensure data is not tampered with during storage or transmission.\n",
      "        Good: Using checksums or digital signatures to verify data integrity.\n",
      "        Bad: No mechanisms in place to check if data has been altered or corrupted.\n",
      "        Least Privilege Access: Implement the principle of least privilege across the codebase.\n",
      "        Good: Assigning minimal required permissions for database access, file operations, and API calls.\n",
      "        Bad: Using a single admin-level account for all database operations, irrespective of the operation's sensitivity.\n",
      "        Logging and Monitoring: Keep detailed logs for security-relevant events and set up automated monitoring and alerts for unusual activities.\n",
      "        Good: Logging failed login attempts and setting up alerts for multiple failures from the same IP.\n",
      "        Bad: No logging or monitoring in place for security-relevant activities.\n",
      "LLM Response:\n",
      "Initial thoughts:\n",
      "- The code seems to be written in Python and is focused on handling NaN values in a dataset.\n",
      "- The code includes functions for handling edge cases, finding subsets of NaN values, testing data conservation, filling NaN values, and converting data back to a DataFrame.\n",
      "- The code lacks proper documentation, making it difficult to understand the purpose and functionality of each function.\n",
      "\n",
      "Numerical response: 6/10\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intuitive_design': 8,\n",
       " 'functional_cohesion': 7,\n",
       " 'adaptive_resilience': 4,\n",
       " 'code_efficiency': 6,\n",
       " 'data_security_and_integrity': 6}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_maintainability_metrics('test/testfile.py', code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "- The code seems to be well-structured with clear variable and function names.\n",
      "- The comments are helpful in understanding the purpose of the code.\n",
      "- The code could benefit from more comprehensive error handling and testing.\n",
      "\n",
      "Overall, I would rate this code a 7/10.\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "- The code seems to be implementing a data cleaning process for handling missing values in a dataset.\n",
      "- The code is well-structured with functions that have clear responsibilities.\n",
      "- The use of numpy and pandas libraries indicates efficient data manipulation and analysis.\n",
      "\n",
      "Overall, I would rate this code a 7/10. It demonstrates good functional cohesion and follows the Single Responsibility Principle. However, there is room for improvement in terms of code readability and documentation.\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "- The code lacks comprehensive error handling mechanisms. There are no try-except blocks to handle potential exceptions in various operations.\n",
      "- The code does not demonstrate graceful degradation. There is no fallback mechanism in case of failure in dependent services.\n",
      "- Resource management is not explicitly handled. There are no context managers used for file operations, database connections, or thread locks.\n",
      "\n",
      "Overall Score: 4/10\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "- The code seems to be written in Python and uses libraries such as pandas and numpy for data manipulation and analysis.\n",
      "- The code includes several functions that perform different tasks such as handling null values, finding subsets of null values, and filling null values.\n",
      "- The code lacks proper documentation and comments, making it difficult to understand the purpose and functionality of each function.\n",
      "\n",
      "Overall, I would rate the code a 6/10. While it seems to perform the required tasks, the lack of documentation and comments makes it difficult to evaluate its efficiency and correctness. Additionally, there is no information provided about the expected input and output of each function, which further hinders the code review process.\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Initial thoughts:\n",
      "- The code seems to be written in Python and is focused on handling NaN values in a dataset.\n",
      "- The code includes several functions that perform different tasks such as finding subsets of NaN values, filling NaN values, and performing tests.\n",
      "- The code lacks proper documentation and comments, making it difficult to understand the purpose and functionality of each function.\n",
      "\n",
      "Numerical response: 6/10\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intuitive_design': 7,\n",
       " 'functional_cohesion': 7,\n",
       " 'adaptive_resilience': 4,\n",
       " 'code_efficiency': 6,\n",
       " 'data_security_and_integrity': 6}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_maintainability_metrics('test/testfile.py', code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
